# ğŸ” AnÃ¡lisis de Servicios de IA y su Impacto en Velocidad

**Fecha**: 2025-12-26  
**Hallazgo**: El proyecto usa **mÃºltiples proveedores de IA**  
**Impacto**: Diferentes latencias segÃºn el servicio

---

## ğŸ“Š Servicios de IA Detectados

### 1. **OpenAI** (Cart Agent, Booking Agent)

**Uso**:

- TTS (Text-to-Speech) para voz
- Whisper para STT (Speech-to-Text)
- GPT-4o-mini para conversaciÃ³n

**Latencia tÃ­pica**:

- TTS: 1000-1500ms
- Whisper: 500-800ms
- GPT-4o-mini: 300-600ms

**Archivos**:

- `backend/src/core/infrastructure/ai/openai.provider.ts`
- `backend/src/agents/abandoned-cart/interface/http/voice.controller.ts`

---

### 2. **AWS Bedrock** (Rider Agent)

**Uso**:

- Claude (Anthropic) para interpretaciÃ³n de intents
- Procesamiento de lenguaje natural

**Latencia tÃ­pica**:

- Claude en Bedrock: 800-1200ms (regiÃ³n us-east-1)
- Puede variar segÃºn regiÃ³n AWS

**Archivos**:

- `backend/src/agents/rider-agent/application/services/rider-bedrock.service.ts`
- `backend/src/agents/rider-agent/presentation/rider-agent.controller.ts`

**ConfiguraciÃ³n actual**:

```typescript
// rider-bedrock.service.ts
this.client = new BedrockRuntimeClient({
  region: this.configService.get<string>("AWS_REGION") || "us-east-1",
});

// Modelo usado
modelId: "anthropic.claude-3-haiku-20240307-v1:0";
```

---

### 3. **LangChain** (Detectado)

**Archivo**: `backend/src/core/infrastructure/ai/langchain.provider.ts`

**Uso**: Posiblemente para orquestaciÃ³n de agentes

---

## ğŸ¯ Impacto en Velocidad por Agente

### Cart Agent (OpenAI)

```
Flujo actual:
1. Usuario carga pÃ¡gina: 0ms
2. Delay artificial: 800ms âŒ (ELIMINADO)
3. Llamada OpenAI TTS: 1200ms
4. Descarga audio: 100ms
5. ReproducciÃ³n: 50ms
--------------------------------
TOTAL: 2150ms (antes: 2950ms)
```

**Mejora aplicada**: âœ… -800ms (27% mÃ¡s rÃ¡pido)

---

### Booking Agent (OpenAI)

```
Flujo actual:
1. Usuario selecciona servicio: 0ms
2. Llamada OpenAI TTS: 1200ms
3. Descarga audio: 100ms
4. ReproducciÃ³n: 50ms
--------------------------------
TOTAL: 1350ms
```

**Estado**: âœ… Ya optimizado (sin delays artificiales)

---

### Rider Agent (AWS Bedrock + Browser TTS)

```
Flujo actual:
1. Usuario habla: 0ms
2. Browser STT (Web Speech API): 500ms
3. Llamada Bedrock (Claude): 1000ms
4. Browser TTS (speechSynthesis): 200ms
--------------------------------
TOTAL: 1700ms
```

**Estado**: âš ï¸ Usa Browser TTS (mÃ¡s rÃ¡pido pero menos natural)

**ConsideraciÃ³n importante**:

- âœ… Bedrock NO se usa para voz, solo para NLU
- âœ… La voz usa Browser TTS (instantÃ¡neo)
- âš ï¸ Bedrock podrÃ­a optimizarse cambiando regiÃ³n o modelo

---

## ğŸ“Š ComparaciÃ³n de Latencias

| Servicio               | OperaciÃ³n | Latencia | Calidad    | Costo           |
| ---------------------- | --------- | -------- | ---------- | --------------- |
| **OpenAI TTS**         | Voz       | 1200ms   | â­â­â­â­â­ | $15/1M chars    |
| **Browser TTS**        | Voz       | 200ms    | â­â­       | Gratis          |
| **OpenAI Whisper**     | STT       | 600ms    | â­â­â­â­â­ | $0.006/min      |
| **Browser STT**        | STT       | 500ms    | â­â­â­â­   | Gratis          |
| **Bedrock Claude**     | NLU       | 1000ms   | â­â­â­â­â­ | $0.25/1M tokens |
| **OpenAI GPT-4o-mini** | NLU       | 400ms    | â­â­â­â­   | $0.15/1M tokens |

---

## ğŸš€ Optimizaciones EspecÃ­ficas por Servicio

### Para OpenAI TTS (Cart Agent, Booking Agent)

#### âœ… Ya Implementado:

1. Modelo HD (`tts-1-hd`) para mejor calidad
2. Voz optimizada por agente (`nova`, `echo`)
3. Cache de 24 horas en backend

#### ğŸ¯ Optimizaciones Adicionales Recomendadas:

**1. Pre-generar Saludos EstÃ¡ticos** (ReducciÃ³n: 1200ms â†’ 0ms)

```bash
# Generar archivos MP3 una sola vez
npm run generate:greetings

# Resultado:
# frontend/src/assets/audio/
# â”œâ”€â”€ cart-agent-greeting.mp3  (pre-generado)
# â”œâ”€â”€ booking-agent-greeting.mp3
# â””â”€â”€ rider-agent-greeting.mp3
```

**Impacto**:

- Primera carga: 2150ms â†’ 300ms (86% mÃ¡s rÃ¡pido)
- Cargas subsecuentes: 50ms (cache del navegador)

**2. Streaming TTS** (ReducciÃ³n: 1200ms â†’ 600ms)

```typescript
// Usar streaming para empezar a reproducir antes
const stream = await this.openai.audio.speech.create({
  model: "tts-1-hd",
  voice: "nova",
  input: text,
  response_format: "opus", // Mejor para streaming
});

// Reproducir mientras se descarga
```

**Impacto**: PercepciÃ³n de 50% mÃ¡s rÃ¡pido

---

### Para AWS Bedrock (Rider Agent)

#### âœ… ConfiguraciÃ³n Actual:

```typescript
// Modelo: Claude 3 Haiku (mÃ¡s rÃ¡pido)
modelId: "anthropic.claude-3-haiku-20240307-v1:0";

// RegiÃ³n: us-east-1
region: "us-east-1";
```

#### ğŸ¯ Optimizaciones Recomendadas:

**1. Verificar RegiÃ³n Ã“ptima**

```typescript
// Probar diferentes regiones segÃºn tu ubicaciÃ³n
const regions = {
  "us-east-1": "Virginia (actual)",
  "eu-west-1": "Irlanda (si estÃ¡s en Europa)",
  "us-west-2": "Oregon",
};

// Medir latencia por regiÃ³n
```

**Impacto potencial**: -200ms si estÃ¡s en Europa

**2. Usar Bedrock con Streaming**

```typescript
// Activar streaming para respuestas mÃ¡s rÃ¡pidas
const command = new InvokeModelWithResponseStreamCommand({
  modelId: "anthropic.claude-3-haiku-20240307-v1:0",
  body: JSON.stringify({
    // ...
    stream: true, // âœ… Activar streaming
  }),
});
```

**Impacto**: PercepciÃ³n de 40% mÃ¡s rÃ¡pido

**3. Considerar Modelo MÃ¡s RÃ¡pido**

```typescript
// Claude 3 Haiku es ya el mÃ¡s rÃ¡pido
// Alternativa: Usar cache de prompts
const body = {
  anthropic_version: "bedrock-2023-05-31",
  max_tokens: 100, // âœ… Reducir tokens si es posible
  messages: [
    {
      role: "user",
      content: prompt,
    },
  ],
  system: [
    {
      type: "text",
      text: systemPrompt,
      cache_control: { type: "ephemeral" }, // âœ… Cache de sistema
    },
  ],
};
```

**Impacto**: -30% en latencia para llamadas repetidas

---

### Para Browser TTS/STT (Rider Agent)

#### âœ… Ya Optimizado:

- Usa Web Speech API (instantÃ¡neo)
- Sin llamadas a backend para voz

#### ğŸ¯ Mejora de Calidad (sin afectar velocidad):

**Implementar selecciÃ³n de voz mejorada** (ya sugerido en anÃ¡lisis anterior):

```typescript
// Priorizar voces de mejor calidad
const voicePriorities = [
  (v) => v.lang.includes("es-ES") && v.name.includes("Google"),
  (v) => v.lang.includes("es-ES") && v.name.includes("Monica"),
  (v) => v.lang.includes("es-ES") && v.name.includes("Microsoft"),
  (v) => v.lang.includes("es-ES"),
];
```

---

## ğŸ¯ Estrategia de OptimizaciÃ³n Recomendada

### Fase 1: Quick Wins (Ya Completado) âœ…

- âœ… Eliminar delays artificiales (-800ms)
- âœ… Optimizar voces OpenAI (mejor calidad)
- âœ… Agregar cache de 24h

**Resultado**: Cart Agent 27% mÃ¡s rÃ¡pido

---

### Fase 2: Optimizaciones de Servicio (Recomendado - 4 horas)

#### Para OpenAI (Cart/Booking):

1. **Pre-generar saludos estÃ¡ticos** (2 horas)

   - Impacto: -86% latencia
   - Costo: $0 (una sola generaciÃ³n)

2. **Implementar streaming TTS** (2 horas)
   - Impacto: PercepciÃ³n 50% mÃ¡s rÃ¡pido
   - Complejidad: Media

#### Para Bedrock (Rider):

1. **Verificar regiÃ³n Ã³ptima** (30 min)

   - Impacto: Hasta -200ms
   - Costo: $0

2. **Activar streaming** (1 hora)

   - Impacto: PercepciÃ³n 40% mÃ¡s rÃ¡pido
   - Complejidad: Media

3. **Implementar cache de prompts** (1 hora)
   - Impacto: -30% en llamadas repetidas
   - Costo: ReducciÃ³n de costos

---

### Fase 3: Alternativas de Servicio (Opcional - EvaluaciÃ³n)

#### OpciÃ³n A: Migrar todo a un solo proveedor

**Pros**:

- Consistencia
- MÃ¡s fÃ¡cil de mantener
- Posibles descuentos por volumen

**Cons**:

- Vendor lock-in
- PÃ©rdida de flexibilidad

#### OpciÃ³n B: Mantener arquitectura multi-proveedor

**Pros**:

- Flexibilidad
- Mejor precio/rendimiento por caso de uso
- Redundancia

**Cons**:

- MÃ¡s complejo de mantener
- Diferentes latencias

**RecomendaciÃ³n**: âœ… Mantener multi-proveedor actual

- OpenAI para voz (mejor calidad TTS)
- Bedrock para NLU (mejor precio/rendimiento)
- Browser APIs para casos de uso rÃ¡pidos

---

## ğŸ“Š Resumen de Impacto en Velocidad

### Estado Actual (DespuÃ©s de Fase 1)

| Agente      | Servicio Principal | Latencia | OptimizaciÃ³n  |
| ----------- | ------------------ | -------- | ------------- |
| **Cart**    | OpenAI TTS         | 2150ms   | âœ… -27%       |
| **Booking** | OpenAI TTS         | 1350ms   | âœ… Optimizado |
| **Rider**   | Bedrock + Browser  | 1700ms   | âš ï¸ Pendiente  |

### Potencial con Fase 2

| Agente      | Latencia Actual | Latencia Potencial | Mejora      |
| ----------- | --------------- | ------------------ | ----------- |
| **Cart**    | 2150ms          | 300ms              | **-86%** ğŸš€ |
| **Booking** | 1350ms          | 300ms              | **-78%** ğŸš€ |
| **Rider**   | 1700ms          | 1200ms             | **-29%** ğŸ“ˆ |

---

## ğŸ¯ RecomendaciÃ³n Final

### Para tus servicios de IA:

1. **OpenAI (Cart/Booking)**:

   - âœ… Mantener para TTS (mejor calidad)
   - ğŸš€ Implementar pre-generaciÃ³n de saludos
   - ğŸ“ˆ Considerar streaming para mensajes largos

2. **Bedrock (Rider)**:

   - âœ… Mantener para NLU (buen precio/rendimiento)
   - ğŸ” Verificar regiÃ³n Ã³ptima
   - ğŸ“ˆ Activar streaming y cache

3. **Browser APIs (Rider)**:
   - âœ… Mantener para voz (velocidad)
   - ğŸ“ˆ Mejorar selecciÃ³n de voces

### Prioridad de implementaciÃ³n:

1. ğŸ”´ **Alta**: Pre-generar saludos OpenAI (mayor impacto)
2. ğŸŸ¡ **Media**: Optimizar regiÃ³n Bedrock
3. ğŸŸ¢ **Baja**: Streaming (mejora percepciÃ³n, no latencia real)

---

**ConclusiÃ³n**: Tus servicios de IA estÃ¡n bien elegidos. Bedrock NO afecta la velocidad de voz (usa Browser TTS). La mayor oportunidad de optimizaciÃ³n estÃ¡ en **pre-generar los saludos de OpenAI**.

Â¿Quieres que implemente la pre-generaciÃ³n de saludos estÃ¡ticos ahora?
